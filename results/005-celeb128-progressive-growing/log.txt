Importing Theano...
/home/yyh/miniconda2/lib/python2.7/site-packages/theano/configdefaults.py:1931: UserWarning: Theano does not recognise this flag: lib.cnmem
  warnings.warn('Theano does not recognise this flag: {0}'.format(key))
/home/yyh/miniconda2/lib/python2.7/site-packages/theano/configdefaults.py:1931: UserWarning: Theano does not recognise this flag: nvcc.fastmath
  warnings.warn('Theano does not recognise this flag: {0}'.format(key))
Using cuDNN version 5103 on context None
Preallocating 8937/11172 Mb (0.800000) on cuda
Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:4B:00.0)
/home/yyh/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

LayerName       LayerType                   Params    OutputShape         WeightShape         Activation
---             ---                         ---       ---                 ---                 ---
Glatents        InputLayer                  0         ?, 128                                  
Ginb            ReshapeLayer                0         ?, 128, 1, 1                            
G1a             Conv2DLayer                 1048576   ?, 512, 4, 4        512, 128, 4, 4      linear
G1a_bn          BatchNormLayer              1024      ?, 512, 4, 4                            
G1a_bn_nonlin   NonlinearityLayer           0         ?, 512, 4, 4                            rectify
G1b             Conv2DLayer                 2359296   ?, 512, 4, 4        512, 512, 3, 3      linear
G1b_bn          BatchNormLayer              1024      ?, 512, 4, 4                            
G1b_bn_nonlin   NonlinearityLayer           0         ?, 512, 4, 4                            rectify
G2up            Upscale2DLayer              0         ?, 512, 8, 8                            
G2a             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
G2a_bn          BatchNormLayer              1024      ?, 512, 8, 8                            
G2a_bn_nonlin   NonlinearityLayer           0         ?, 512, 8, 8                            rectify
G2b             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
G2b_bn          BatchNormLayer              1024      ?, 512, 8, 8                            
G2b_bn_nonlin   NonlinearityLayer           0         ?, 512, 8, 8                            rectify
G3up            Upscale2DLayer              0         ?, 512, 16, 16                          
G3a             Conv2DLayer                 1179648   ?, 256, 16, 16      256, 512, 3, 3      linear
G3a_bn          BatchNormLayer              512       ?, 256, 16, 16                          
G3a_bn_nonlin   NonlinearityLayer           0         ?, 256, 16, 16                          rectify
G3b             Conv2DLayer                 589824    ?, 256, 16, 16      256, 256, 3, 3      linear
G3b_bn          BatchNormLayer              512       ?, 256, 16, 16                          
G3b_bn_nonlin   NonlinearityLayer           0         ?, 256, 16, 16                          rectify
G4up            Upscale2DLayer              0         ?, 256, 32, 32                          
G4a             Conv2DLayer                 294912    ?, 128, 32, 32      128, 256, 3, 3      linear
G4a_bn          BatchNormLayer              256       ?, 128, 32, 32                          
G4a_bn_nonlin   NonlinearityLayer           0         ?, 128, 32, 32                          rectify
G4b             Conv2DLayer                 147456    ?, 128, 32, 32      128, 128, 3, 3      linear
G4b_bn          BatchNormLayer              256       ?, 128, 32, 32                          
G4b_bn_nonlin   NonlinearityLayer           0         ?, 128, 32, 32                          rectify
G5up            Upscale2DLayer              0         ?, 128, 64, 64                          
G5a             Conv2DLayer                 73728     ?, 64, 64, 64       64, 128, 3, 3       linear
G5a_bn          BatchNormLayer              128       ?, 64, 64, 64                           
G5a_bn_nonlin   NonlinearityLayer           0         ?, 64, 64, 64                           rectify
G5b             Conv2DLayer                 36864     ?, 64, 64, 64       64, 64, 3, 3        linear
G5b_bn          BatchNormLayer              128       ?, 64, 64, 64                           
G5b_bn_nonlin   NonlinearityLayer           0         ?, 64, 64, 64                           rectify
G6up            Upscale2DLayer              0         ?, 64, 128, 128                         
G6a             Conv2DLayer                 18432     ?, 32, 128, 128     32, 64, 3, 3        linear
G6a_bn          BatchNormLayer              64        ?, 32, 128, 128                         
G6a_bn_nonlin   NonlinearityLayer           0         ?, 32, 128, 128                         rectify
G6b             Conv2DLayer                 9216      ?, 32, 128, 128     32, 32, 3, 3        linear
G6b_bn          BatchNormLayer              64        ?, 32, 128, 128                         
G6b_bn_nonlin   NonlinearityLayer           0         ?, 32, 128, 128                         rectify
Glod0           NINLayer                    99        ?, 3, 128, 128      32, 3               linear
Glod1           NINLayer                    195       ?, 3, 64, 64        64, 3               linear
Glod2           NINLayer                    387       ?, 3, 32, 32        128, 3              linear
Glod3           NINLayer                    771       ?, 3, 16, 16        256, 3              linear
Glod4           NINLayer                    1539      ?, 3, 8, 8          512, 3              linear
Glod5           NINLayer                    1539      ?, 3, 4, 4          512, 3              linear
Glod            LODSelectLayer              0         ?, 3, 128, 128                          
Gtanh           NonlinearityLayer           0         ?, 3, 128, 128                          tanh
---             ---                         ---       ---                 ---                 ---
Total                                       10487090                                          


LayerName       LayerType                   Params    OutputShape         WeightShape         Activation
---             ---                         ---       ---                 ---                 ---
Dimages         InputLayer                  0         ?, 3, 128, 128                          
D6x             NINLayer                    128       ?, 32, 128, 128     3, 32               LeakyRectify
D6b             Conv2DLayer                 9216      ?, 32, 128, 128     32, 32, 3, 3        linear
D6bln           LayerNormLayer              33        ?, 32, 128, 128                         LeakyRectify
D6a             Conv2DLayer                 18432     ?, 64, 128, 128     64, 32, 3, 3        linear
D6aln           LayerNormLayer              65        ?, 64, 128, 128                         LeakyRectify
D6dn            Pool2DLayer                 0         ?, 64, 64, 64                           
D5xs            Pool2DLayer                 0         ?, 3, 64, 64                            
D5x             NINLayer                    256       ?, 64, 64, 64       3, 64               LeakyRectify
D5lod           LODSelectLayer              0         ?, 64, 64, 64                           
D5b             Conv2DLayer                 36864     ?, 64, 64, 64       64, 64, 3, 3        linear
D5bln           LayerNormLayer              65        ?, 64, 64, 64                           LeakyRectify
D5a             Conv2DLayer                 73728     ?, 128, 64, 64      128, 64, 3, 3       linear
D5aln           LayerNormLayer              129       ?, 128, 64, 64                          LeakyRectify
D5dn            Pool2DLayer                 0         ?, 128, 32, 32                          
D4xs            Pool2DLayer                 0         ?, 3, 32, 32                            
D4x             NINLayer                    512       ?, 128, 32, 32      3, 128              LeakyRectify
D4lod           LODSelectLayer              0         ?, 128, 32, 32                          
D4b             Conv2DLayer                 147456    ?, 128, 32, 32      128, 128, 3, 3      linear
D4bln           LayerNormLayer              129       ?, 128, 32, 32                          LeakyRectify
D4a             Conv2DLayer                 294912    ?, 256, 32, 32      256, 128, 3, 3      linear
D4aln           LayerNormLayer              257       ?, 256, 32, 32                          LeakyRectify
D4dn            Pool2DLayer                 0         ?, 256, 16, 16                          
D3xs            Pool2DLayer                 0         ?, 3, 16, 16                            
D3x             NINLayer                    1024      ?, 256, 16, 16      3, 256              LeakyRectify
D3lod           LODSelectLayer              0         ?, 256, 16, 16                          
D3b             Conv2DLayer                 589824    ?, 256, 16, 16      256, 256, 3, 3      linear
D3bln           LayerNormLayer              257       ?, 256, 16, 16                          LeakyRectify
D3a             Conv2DLayer                 1179648   ?, 512, 16, 16      512, 256, 3, 3      linear
D3aln           LayerNormLayer              513       ?, 512, 16, 16                          LeakyRectify
D3dn            Pool2DLayer                 0         ?, 512, 8, 8                            
D2xs            Pool2DLayer                 0         ?, 3, 8, 8                              
D2x             NINLayer                    2048      ?, 512, 8, 8        3, 512              LeakyRectify
D2lod           LODSelectLayer              0         ?, 512, 8, 8                            
D2b             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
D2bln           LayerNormLayer              513       ?, 512, 8, 8                            LeakyRectify
D2a             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
D2aln           LayerNormLayer              513       ?, 512, 8, 8                            LeakyRectify
D2dn            Pool2DLayer                 0         ?, 512, 4, 4                            
D1xs            Pool2DLayer                 0         ?, 3, 4, 4                              
D1x             NINLayer                    2048      ?, 512, 4, 4        3, 512              LeakyRectify
D1lod           LODSelectLayer              0         ?, 512, 4, 4                            
D1b             Conv2DLayer                 2359296   ?, 512, 4, 4        512, 512, 3, 3      linear
D1bln           LayerNormLayer              513       ?, 512, 4, 4                            LeakyRectify
D1a             Conv2DLayer                 4194304   ?, 512, 1, 1        512, 512, 4, 4      linear
D1aln           LayerNormLayer              513       ?, 512, 1, 1                            LeakyRectify
Dscores         DenseLayer                  513       ?, 1                512, 1              linear
---             ---                         ---       ---                 ---                 ---
Total                                       13632301                                          

Setting up Theano...
Saving results to results/005-celeb128-progressive-growing
Compiling training funcs...
tick 1     kimg 50.2     lod 5.00  minibatch 64   time 54s          sec/tick 54.5      sec/kimg 1.1    Dgdrop 0.0000   Gloss 20.5834  Dloss -32.8763 Dreal 0.0000   Dfake -37.9410
tick 2     kimg 100.5    lod 5.00  minibatch 64   time 1m 25s       sec/tick 30.5      sec/kimg 0.6    Dgdrop 0.0000   Gloss 28.2838  Dloss -46.0225 Dreal 0.0000   Dfake -56.9243
tick 3     kimg 150.7    lod 5.00  minibatch 64   time 1m 56s       sec/tick 30.6      sec/kimg 0.6    Dgdrop 0.0000   Gloss 22.4749  Dloss -27.9104 Dreal 0.0000   Dfake -37.7521
tick 4     kimg 201.0    lod 5.00  minibatch 64   time 2m 26s       sec/tick 30.8      sec/kimg 0.6    Dgdrop 0.0000   Gloss 26.4322  Dloss -18.3127 Dreal 0.0000   Dfake -24.0768
tick 5     kimg 251.2    lod 5.00  minibatch 64   time 2m 58s       sec/tick 31.2      sec/kimg 0.6    Dgdrop 0.0000   Gloss 31.6596  Dloss -14.3388 Dreal 0.0000   Dfake -18.3940
tick 6     kimg 301.4    lod 5.00  minibatch 64   time 3m 30s       sec/tick 32.2      sec/kimg 0.6    Dgdrop 0.0000   Gloss 37.0526  Dloss -12.8269 Dreal 0.0000   Dfake -16.2714
tick 7     kimg 351.7    lod 5.00  minibatch 64   time 4m 03s       sec/tick 33.0      sec/kimg 0.7    Dgdrop 0.0000   Gloss 44.2848  Dloss -11.6982 Dreal 0.0000   Dfake -14.8770
tick 8     kimg 401.9    lod 5.00  minibatch 64   time 4m 36s       sec/tick 33.5      sec/kimg 0.7    Dgdrop 0.0000   Gloss 50.4584  Dloss -11.0942 Dreal 0.0000   Dfake -14.1607
tick 9     kimg 452.2    lod 5.00  minibatch 64   time 5m 10s       sec/tick 34.1      sec/kimg 0.7    Dgdrop 0.0000   Gloss 57.4044  Dloss -10.0992 Dreal 0.0000   Dfake -12.9430
tick 10    kimg 502.4    lod 5.00  minibatch 64   time 5m 44s       sec/tick 34.0      sec/kimg 0.7    Dgdrop 0.0000   Gloss 63.6877  Dloss -9.7576  Dreal 0.0000   Dfake -12.4593
tick 11    kimg 552.6    lod 5.00  minibatch 64   time 6m 18s       sec/tick 34.0      sec/kimg 0.7    Dgdrop 0.0000   Gloss 72.0575  Dloss -9.3326  Dreal 0.0000   Dfake -11.9469
tick 12    kimg 602.9    lod 5.00  minibatch 64   time 6m 52s       sec/tick 34.1      sec/kimg 0.7    Dgdrop 0.0000   Gloss 75.2233  Dloss -8.3098  Dreal 0.0000   Dfake -10.6379
tick 13    kimg 653.1    lod 5.00  minibatch 64   time 7m 27s       sec/tick 34.5      sec/kimg 0.7    Dgdrop 0.0000   Gloss 79.2228  Dloss -7.6613  Dreal 0.0000   Dfake -9.8240 
tick 14    kimg 703.4    lod 5.00  minibatch 64   time 8m 01s       sec/tick 34.2      sec/kimg 0.7    Dgdrop 0.0000   Gloss 82.4511  Dloss -6.9934  Dreal 0.0000   Dfake -8.9227 
tick 15    kimg 753.6    lod 5.00  minibatch 64   time 8m 35s       sec/tick 34.2      sec/kimg 0.7    Dgdrop 0.0000   Gloss 83.2919  Dloss -6.6257  Dreal 0.0000   Dfake -8.3989 
Compiling training funcs...
tick 16    kimg 803.8    lod 5.00  minibatch 64   time 10m 02s      sec/tick 86.6      sec/kimg 1.7    Dgdrop 0.0000   Gloss 84.6107  Dloss -6.2529  Dreal 0.0000   Dfake -7.9579 
tick 17    kimg 854.1    lod 4.93  minibatch 64   time 12m 26s      sec/tick 144.3     sec/kimg 2.9    Dgdrop 0.0000   Gloss 86.7798  Dloss -6.2128  Dreal 0.0000   Dfake -7.8519 
tick 18    kimg 904.3    lod 4.87  minibatch 64   time 14m 53s      sec/tick 146.4     sec/kimg 2.9    Dgdrop 0.0000   Gloss 89.2935  Dloss -5.8578  Dreal 0.0000   Dfake -7.4781 
tick 19    kimg 954.6    lod 4.81  minibatch 64   time 17m 18s      sec/tick 145.8     sec/kimg 2.9    Dgdrop 0.0000   Gloss 88.1662  Dloss -5.6860  Dreal 0.0000   Dfake -7.2340 
Traceback (most recent call last):
  File "train.py", line 676, in <module>
    globals()[func_name](**func_params)
  File "train.py", line 263, in train_gan
    mb_train_out = D_train_fn(mb_reals, mb_labels, random_latents(minibatch_size, G.input_shape), random_labels(minibatch_size, training_set))
  File "/home/yyh/miniconda2/lib/python2.7/site-packages/theano/compile/function_module.py", line 903, in __call__
    self.fn() if output_subset is None else\
  File "/home/yyh/miniconda2/lib/python2.7/site-packages/theano/ifelse.py", line 245, in thunk
    def thunk():
KeyboardInterrupt
