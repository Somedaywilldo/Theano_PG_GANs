Importing Theano...
/home/yyh/miniconda2/lib/python2.7/site-packages/theano/configdefaults.py:1931: UserWarning: Theano does not recognise this flag: lib.cnmem
  warnings.warn('Theano does not recognise this flag: {0}'.format(key))
/home/yyh/miniconda2/lib/python2.7/site-packages/theano/configdefaults.py:1931: UserWarning: Theano does not recognise this flag: nvcc.fastmath
  warnings.warn('Theano does not recognise this flag: {0}'.format(key))
Using cuDNN version 5103 on context None
Preallocating 8937/11172 Mb (0.800000) on cuda
Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:4B:00.0)
/home/yyh/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

LayerName       LayerType                   Params    OutputShape         WeightShape         Activation
---             ---                         ---       ---                 ---                 ---
Glatents        InputLayer                  0         ?, 128                                  
Ginb            ReshapeLayer                0         ?, 128, 1, 1                            
G1a             Conv2DLayer                 1048576   ?, 512, 4, 4        512, 128, 4, 4      linear
G1a_bn          BatchNormLayer              1024      ?, 512, 4, 4                            
G1a_bn_nonlin   NonlinearityLayer           0         ?, 512, 4, 4                            rectify
G1b             Conv2DLayer                 2359296   ?, 512, 4, 4        512, 512, 3, 3      linear
G1b_bn          BatchNormLayer              1024      ?, 512, 4, 4                            
G1b_bn_nonlin   NonlinearityLayer           0         ?, 512, 4, 4                            rectify
G2up            Upscale2DLayer              0         ?, 512, 8, 8                            
G2a             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
G2a_bn          BatchNormLayer              1024      ?, 512, 8, 8                            
G2a_bn_nonlin   NonlinearityLayer           0         ?, 512, 8, 8                            rectify
G2b             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
G2b_bn          BatchNormLayer              1024      ?, 512, 8, 8                            
G2b_bn_nonlin   NonlinearityLayer           0         ?, 512, 8, 8                            rectify
G3up            Upscale2DLayer              0         ?, 512, 16, 16                          
G3a             Conv2DLayer                 1179648   ?, 256, 16, 16      256, 512, 3, 3      linear
G3a_bn          BatchNormLayer              512       ?, 256, 16, 16                          
G3a_bn_nonlin   NonlinearityLayer           0         ?, 256, 16, 16                          rectify
G3b             Conv2DLayer                 589824    ?, 256, 16, 16      256, 256, 3, 3      linear
G3b_bn          BatchNormLayer              512       ?, 256, 16, 16                          
G3b_bn_nonlin   NonlinearityLayer           0         ?, 256, 16, 16                          rectify
G4up            Upscale2DLayer              0         ?, 256, 32, 32                          
G4a             Conv2DLayer                 294912    ?, 128, 32, 32      128, 256, 3, 3      linear
G4a_bn          BatchNormLayer              256       ?, 128, 32, 32                          
G4a_bn_nonlin   NonlinearityLayer           0         ?, 128, 32, 32                          rectify
G4b             Conv2DLayer                 147456    ?, 128, 32, 32      128, 128, 3, 3      linear
G4b_bn          BatchNormLayer              256       ?, 128, 32, 32                          
G4b_bn_nonlin   NonlinearityLayer           0         ?, 128, 32, 32                          rectify
G5up            Upscale2DLayer              0         ?, 128, 64, 64                          
G5a             Conv2DLayer                 73728     ?, 64, 64, 64       64, 128, 3, 3       linear
G5a_bn          BatchNormLayer              128       ?, 64, 64, 64                           
G5a_bn_nonlin   NonlinearityLayer           0         ?, 64, 64, 64                           rectify
G5b             Conv2DLayer                 36864     ?, 64, 64, 64       64, 64, 3, 3        linear
G5b_bn          BatchNormLayer              128       ?, 64, 64, 64                           
G5b_bn_nonlin   NonlinearityLayer           0         ?, 64, 64, 64                           rectify
G6up            Upscale2DLayer              0         ?, 64, 128, 128                         
G6a             Conv2DLayer                 18432     ?, 32, 128, 128     32, 64, 3, 3        linear
G6a_bn          BatchNormLayer              64        ?, 32, 128, 128                         
G6a_bn_nonlin   NonlinearityLayer           0         ?, 32, 128, 128                         rectify
G6b             Conv2DLayer                 9216      ?, 32, 128, 128     32, 32, 3, 3        linear
G6b_bn          BatchNormLayer              64        ?, 32, 128, 128                         
G6b_bn_nonlin   NonlinearityLayer           0         ?, 32, 128, 128                         rectify
Glod0           NINLayer                    99        ?, 3, 128, 128      32, 3               linear
Glod1           NINLayer                    195       ?, 3, 64, 64        64, 3               linear
Glod2           NINLayer                    387       ?, 3, 32, 32        128, 3              linear
Glod3           NINLayer                    771       ?, 3, 16, 16        256, 3              linear
Glod4           NINLayer                    1539      ?, 3, 8, 8          512, 3              linear
Glod5           NINLayer                    1539      ?, 3, 4, 4          512, 3              linear
Glod            LODSelectLayer              0         ?, 3, 128, 128                          
Gtanh           NonlinearityLayer           0         ?, 3, 128, 128                          tanh
---             ---                         ---       ---                 ---                 ---
Total                                       10487090                                          


LayerName       LayerType                   Params    OutputShape         WeightShape         Activation
---             ---                         ---       ---                 ---                 ---
Dimages         InputLayer                  0         ?, 3, 128, 128                          
D6x             NINLayer                    128       ?, 32, 128, 128     3, 32               LeakyRectify
D6b             Conv2DLayer                 9216      ?, 32, 128, 128     32, 32, 3, 3        linear
D6bln           LayerNormLayer              33        ?, 32, 128, 128                         LeakyRectify
D6a             Conv2DLayer                 18432     ?, 64, 128, 128     64, 32, 3, 3        linear
D6aln           LayerNormLayer              65        ?, 64, 128, 128                         LeakyRectify
D6dn            Pool2DLayer                 0         ?, 64, 64, 64                           
D5xs            Pool2DLayer                 0         ?, 3, 64, 64                            
D5x             NINLayer                    256       ?, 64, 64, 64       3, 64               LeakyRectify
D5lod           LODSelectLayer              0         ?, 64, 64, 64                           
D5b             Conv2DLayer                 36864     ?, 64, 64, 64       64, 64, 3, 3        linear
D5bln           LayerNormLayer              65        ?, 64, 64, 64                           LeakyRectify
D5a             Conv2DLayer                 73728     ?, 128, 64, 64      128, 64, 3, 3       linear
D5aln           LayerNormLayer              129       ?, 128, 64, 64                          LeakyRectify
D5dn            Pool2DLayer                 0         ?, 128, 32, 32                          
D4xs            Pool2DLayer                 0         ?, 3, 32, 32                            
D4x             NINLayer                    512       ?, 128, 32, 32      3, 128              LeakyRectify
D4lod           LODSelectLayer              0         ?, 128, 32, 32                          
D4b             Conv2DLayer                 147456    ?, 128, 32, 32      128, 128, 3, 3      linear
D4bln           LayerNormLayer              129       ?, 128, 32, 32                          LeakyRectify
D4a             Conv2DLayer                 294912    ?, 256, 32, 32      256, 128, 3, 3      linear
D4aln           LayerNormLayer              257       ?, 256, 32, 32                          LeakyRectify
D4dn            Pool2DLayer                 0         ?, 256, 16, 16                          
D3xs            Pool2DLayer                 0         ?, 3, 16, 16                            
D3x             NINLayer                    1024      ?, 256, 16, 16      3, 256              LeakyRectify
D3lod           LODSelectLayer              0         ?, 256, 16, 16                          
D3b             Conv2DLayer                 589824    ?, 256, 16, 16      256, 256, 3, 3      linear
D3bln           LayerNormLayer              257       ?, 256, 16, 16                          LeakyRectify
D3a             Conv2DLayer                 1179648   ?, 512, 16, 16      512, 256, 3, 3      linear
D3aln           LayerNormLayer              513       ?, 512, 16, 16                          LeakyRectify
D3dn            Pool2DLayer                 0         ?, 512, 8, 8                            
D2xs            Pool2DLayer                 0         ?, 3, 8, 8                              
D2x             NINLayer                    2048      ?, 512, 8, 8        3, 512              LeakyRectify
D2lod           LODSelectLayer              0         ?, 512, 8, 8                            
D2b             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
D2bln           LayerNormLayer              513       ?, 512, 8, 8                            LeakyRectify
D2a             Conv2DLayer                 2359296   ?, 512, 8, 8        512, 512, 3, 3      linear
D2aln           LayerNormLayer              513       ?, 512, 8, 8                            LeakyRectify
D2dn            Pool2DLayer                 0         ?, 512, 4, 4                            
D1xs            Pool2DLayer                 0         ?, 3, 4, 4                              
D1x             NINLayer                    2048      ?, 512, 4, 4        3, 512              LeakyRectify
D1lod           LODSelectLayer              0         ?, 512, 4, 4                            
D1b             Conv2DLayer                 2359296   ?, 512, 4, 4        512, 512, 3, 3      linear
D1bln           LayerNormLayer              513       ?, 512, 4, 4                            LeakyRectify
D1a             Conv2DLayer                 4194304   ?, 512, 1, 1        512, 512, 4, 4      linear
D1aln           LayerNormLayer              513       ?, 512, 1, 1                            LeakyRectify
Dscores         DenseLayer                  513       ?, 1                512, 1              linear
---             ---                         ---       ---                 ---                 ---
Total                                       13632301                                          

Setting up Theano...
Saving results to results/006-celeb128-progressive-growing
Compiling training funcs...
tick 1     kimg 50.2     lod 5.00  minibatch 64   time 55s          sec/tick 54.7      sec/kimg 1.1    Dgdrop 0.0000   Gloss 20.5834  Dloss -32.8763 Dreal 0.0000   Dfake -37.9410
tick 2     kimg 100.5    lod 5.00  minibatch 64   time 1m 26s       sec/tick 31.1      sec/kimg 0.6    Dgdrop 0.0000   Gloss 28.2838  Dloss -46.0225 Dreal 0.0000   Dfake -56.9243
tick 3     kimg 150.7    lod 5.00  minibatch 64   time 1m 58s       sec/tick 32.3      sec/kimg 0.6    Dgdrop 0.0000   Gloss 22.4749  Dloss -27.9104 Dreal 0.0000   Dfake -37.7521
tick 4     kimg 201.0    lod 5.00  minibatch 64   time 2m 32s       sec/tick 33.7      sec/kimg 0.7    Dgdrop 0.0000   Gloss 26.4322  Dloss -18.3127 Dreal 0.0000   Dfake -24.0768
tick 5     kimg 251.2    lod 5.00  minibatch 64   time 3m 06s       sec/tick 34.4      sec/kimg 0.7    Dgdrop 0.0000   Gloss 31.6596  Dloss -14.3388 Dreal 0.0000   Dfake -18.3940
tick 6     kimg 301.4    lod 5.00  minibatch 64   time 3m 41s       sec/tick 34.4      sec/kimg 0.7    Dgdrop 0.0000   Gloss 37.0526  Dloss -12.8269 Dreal 0.0000   Dfake -16.2714
tick 7     kimg 351.7    lod 5.00  minibatch 64   time 4m 15s       sec/tick 34.7      sec/kimg 0.7    Dgdrop 0.0000   Gloss 44.2848  Dloss -11.6982 Dreal 0.0000   Dfake -14.8770
tick 8     kimg 401.9    lod 5.00  minibatch 64   time 4m 50s       sec/tick 34.6      sec/kimg 0.7    Dgdrop 0.0000   Gloss 50.4584  Dloss -11.0942 Dreal 0.0000   Dfake -14.1607
tick 9     kimg 452.2    lod 5.00  minibatch 64   time 5m 25s       sec/tick 35.0      sec/kimg 0.7    Dgdrop 0.0000   Gloss 57.4044  Dloss -10.0992 Dreal 0.0000   Dfake -12.9430
tick 10    kimg 502.4    lod 5.00  minibatch 64   time 5m 59s       sec/tick 34.6      sec/kimg 0.7    Dgdrop 0.0000   Gloss 63.6877  Dloss -9.7576  Dreal 0.0000   Dfake -12.4593
tick 11    kimg 552.6    lod 5.00  minibatch 64   time 6m 34s       sec/tick 34.9      sec/kimg 0.7    Dgdrop 0.0000   Gloss 72.0575  Dloss -9.3326  Dreal 0.0000   Dfake -11.9469
tick 12    kimg 602.9    lod 5.00  minibatch 64   time 7m 09s       sec/tick 35.0      sec/kimg 0.7    Dgdrop 0.0000   Gloss 75.2233  Dloss -8.3098  Dreal 0.0000   Dfake -10.6379
tick 13    kimg 653.1    lod 5.00  minibatch 64   time 7m 44s       sec/tick 35.0      sec/kimg 0.7    Dgdrop 0.0000   Gloss 79.2228  Dloss -7.6613  Dreal 0.0000   Dfake -9.8240 
tick 14    kimg 703.4    lod 5.00  minibatch 64   time 8m 19s       sec/tick 34.9      sec/kimg 0.7    Dgdrop 0.0000   Gloss 82.4511  Dloss -6.9934  Dreal 0.0000   Dfake -8.9227 
tick 15    kimg 753.6    lod 5.00  minibatch 64   time 8m 54s       sec/tick 34.8      sec/kimg 0.7    Dgdrop 0.0000   Gloss 83.2919  Dloss -6.6257  Dreal 0.0000   Dfake -8.3989 
Compiling training funcs...
tick 16    kimg 803.8    lod 5.00  minibatch 64   time 10m 19s      sec/tick 84.8      sec/kimg 1.7    Dgdrop 0.0000   Gloss 84.6107  Dloss -6.2529  Dreal 0.0000   Dfake -7.9579 
tick 17    kimg 854.1    lod 4.93  minibatch 64   time 12m 43s      sec/tick 143.8     sec/kimg 2.9    Dgdrop 0.0000   Gloss 86.7798  Dloss -6.2128  Dreal 0.0000   Dfake -7.8519 
tick 18    kimg 904.3    lod 4.87  minibatch 64   time 15m 08s      sec/tick 145.4     sec/kimg 2.9    Dgdrop 0.0000   Gloss 89.2935  Dloss -5.8578  Dreal 0.0000   Dfake -7.4781 
tick 19    kimg 954.6    lod 4.81  minibatch 64   time 17m 34s      sec/tick 146.2     sec/kimg 2.9    Dgdrop 0.0000   Gloss 88.1662  Dloss -5.6860  Dreal 0.0000   Dfake -7.2340 
tick 20    kimg 1004.8   lod 4.74  minibatch 64   time 19m 59s      sec/tick 145.0     sec/kimg 2.9    Dgdrop 0.0000   Gloss 90.3002  Dloss -5.4561  Dreal 0.0000   Dfake -6.8579 
tick 21    kimg 1055.0   lod 4.68  minibatch 64   time 22m 25s      sec/tick 146.2     sec/kimg 2.9    Dgdrop 0.0000   Gloss 86.3880  Dloss -5.3204  Dreal 0.0000   Dfake -6.6691 
tick 22    kimg 1105.3   lod 4.62  minibatch 64   time 24m 51s      sec/tick 145.4     sec/kimg 2.9    Dgdrop 0.0000   Gloss 85.3965  Dloss -5.4392  Dreal 0.0000   Dfake -6.7793 
tick 23    kimg 1155.5   lod 4.56  minibatch 64   time 27m 17s      sec/tick 146.6     sec/kimg 2.9    Dgdrop 0.0000   Gloss 83.3242  Dloss -5.7104  Dreal 0.0000   Dfake -7.1015 
tick 24    kimg 1205.8   lod 4.49  minibatch 64   time 29m 43s      sec/tick 145.7     sec/kimg 2.9    Dgdrop 0.0000   Gloss 80.4874  Dloss -5.7760  Dreal 0.0000   Dfake -7.1847 
tick 25    kimg 1256.0   lod 4.43  minibatch 64   time 32m 09s      sec/tick 146.1     sec/kimg 2.9    Dgdrop 0.0000   Gloss 78.6463  Dloss -6.0828  Dreal 0.0000   Dfake -7.5728 
tick 26    kimg 1306.2   lod 4.37  minibatch 64   time 34m 35s      sec/tick 145.7     sec/kimg 2.9    Dgdrop 0.0000   Gloss 74.0263  Dloss -6.4502  Dreal 0.0000   Dfake -8.0471 
tick 27    kimg 1356.5   lod 4.30  minibatch 64   time 37m 01s      sec/tick 145.7     sec/kimg 2.9    Dgdrop 0.0000   Gloss 70.1200  Dloss -6.5913  Dreal 0.0000   Dfake -8.2397 
tick 28    kimg 1406.7   lod 4.24  minibatch 64   time 39m 28s      sec/tick 147.2     sec/kimg 2.9    Dgdrop 0.0000   Gloss 69.8242  Dloss -6.0157  Dreal 0.0000   Dfake -7.5223 
tick 29    kimg 1457.0   lod 4.18  minibatch 64   time 41m 54s      sec/tick 146.2     sec/kimg 2.9    Dgdrop 0.0000   Gloss 64.6428  Dloss -6.6433  Dreal 0.0000   Dfake -8.3220 
tick 30    kimg 1507.2   lod 4.12  minibatch 64   time 44m 20s      sec/tick 146.5     sec/kimg 2.9    Dgdrop 0.0000   Gloss 59.9979  Dloss -6.6123  Dreal 0.0000   Dfake -8.2382 
tick 31    kimg 1557.4   lod 4.05  minibatch 64   time 46m 46s      sec/tick 146.0     sec/kimg 2.9    Dgdrop 0.0000   Gloss 54.8386  Dloss -6.4798  Dreal 0.0000   Dfake -8.0662 
Compiling training funcs...
tick 32    kimg 1607.7   lod 4.00  minibatch 64   time 49m 47s      sec/tick 180.3     sec/kimg 3.6    Dgdrop 0.0000   Gloss 51.3090  Dloss -6.3462  Dreal 0.0000   Dfake -7.9184 
tick 33    kimg 1657.9   lod 4.00  minibatch 64   time 52m 13s      sec/tick 146.2     sec/kimg 2.9    Dgdrop 0.0000   Gloss 51.1017  Dloss -5.8213  Dreal 0.0000   Dfake -7.2356 
tick 34    kimg 1708.2   lod 4.00  minibatch 64   time 54m 39s      sec/tick 146.3     sec/kimg 2.9    Dgdrop 0.0000   Gloss 49.8027  Dloss -5.3028  Dreal 0.0000   Dfake -6.5692 
tick 35    kimg 1758.4   lod 4.00  minibatch 64   time 57m 04s      sec/tick 145.0     sec/kimg 2.9    Dgdrop 0.0000   Gloss 43.7383  Dloss -4.8384  Dreal 0.0000   Dfake -5.9596 
tick 36    kimg 1808.6   lod 4.00  minibatch 64   time 59m 30s      sec/tick 146.2     sec/kimg 2.9    Dgdrop 0.0000   Gloss 44.4738  Dloss -4.6741  Dreal 0.0000   Dfake -5.7543 
tick 37    kimg 1858.9   lod 4.00  minibatch 64   time 1h 01m 57s   sec/tick 146.7     sec/kimg 2.9    Dgdrop 0.0000   Gloss 44.8504  Dloss -4.5373  Dreal 0.0000   Dfake -5.5729 
tick 38    kimg 1909.1   lod 4.00  minibatch 64   time 1h 04m 22s   sec/tick 145.4     sec/kimg 2.9    Dgdrop 0.0000   Gloss 42.2894  Dloss -4.3603  Dreal 0.0000   Dfake -5.3644 
tick 39    kimg 1959.4   lod 4.00  minibatch 64   time 1h 06m 48s   sec/tick 145.7     sec/kimg 2.9    Dgdrop 0.0000   Gloss 44.8110  Dloss -4.0295  Dreal 0.0000   Dfake -4.9565 
tick 40    kimg 2009.6   lod 4.00  minibatch 64   time 1h 09m 15s   sec/tick 146.4     sec/kimg 2.9    Dgdrop 0.0000   Gloss 44.8754  Dloss -4.0300  Dreal 0.0000   Dfake -4.9526 
tick 41    kimg 2059.8   lod 4.00  minibatch 64   time 1h 11m 41s   sec/tick 146.1     sec/kimg 2.9    Dgdrop 0.0000   Gloss 45.1231  Dloss -3.8788  Dreal 0.0000   Dfake -4.7535 
Traceback (most recent call last):
  File "train.py", line 676, in <module>
    globals()[func_name](**func_params)
  File "train.py", line 263, in train_gan
    mb_train_out = D_train_fn(mb_reals, mb_labels, random_latents(minibatch_size, G.input_shape), random_labels(minibatch_size, training_set))
  File "/home/yyh/miniconda2/lib/python2.7/site-packages/theano/compile/function_module.py", line 903, in __call__
    self.fn() if output_subset is None else\
KeyboardInterrupt
